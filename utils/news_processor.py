"""
æ–°é—»å¢å¼ºå¤„ç†æ¨¡å—
- å…³é”®è¯æå–
- ä¸­æ–‡ç¿»è¯‘ä¸ºè‹±æ–‡ï¼ˆå¯é€‰ï¼‰
- æ™ºèƒ½æ‘˜è¦
- Tokenä¼˜åŒ–
"""

import re
import logging

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ç¿»è¯‘åº“ï¼ˆå¯é€‰ï¼‰
try:
    from googletrans import Translator
    TRANSLATION_AVAILABLE = True
except ImportError:
    TRANSLATION_AVAILABLE = False
    logger.warning("googletransæœªå®‰è£…ï¼Œç¿»è¯‘åŠŸèƒ½ä¸å¯ç”¨")


class NewsProcessor:
    """æ–°é—»å¤„ç†å™¨ - æå–å…³é”®è¯ã€ç¿»è¯‘ã€æ‘˜è¦"""
    
    # åŠ å¯†è´§å¸å…³é”®è¯è¯å…¸
    CRYPTO_KEYWORDS = {
        # ä¸»æµå¸ç§
        'btc', 'bitcoin', 'eth', 'ethereum', 'usdt', 'tether', 'bnb', 'binance',
        'sol', 'solana', 'ada', 'cardano', 'xrp', 'ripple', 'doge', 'dogecoin',
        'matic', 'polygon', 'avax', 'avalanche', 'dot', 'polkadot',
        
        # DeFi/NFT
        'defi', 'nft', 'dao', 'dex', 'cefi', 'web3', 'metaverse', 'gamefi',
        'yield', 'liquidity', 'amm', 'swap', 'uniswap', 'pancakeswap',
        
        # äº¤æ˜“è¡Œä¸º
        'pump', 'dump', 'surge', 'crash', 'rally', 'dip', 'moon', 'bullish', 
        'bearish', 'halving', 'fork', 'airdrop', 'staking', 'mining',
        'whale', 'accumulation', 'distribution',
        
        # é‡‘èæœºæ„
        'sec', 'fed', 'federal reserve', 'treasury', 'cftc', 'finra',
        'blackrock', 'grayscale', 'fidelity', 'vanguard', 'jpmorgan',
        'goldman sachs', 'morgan stanley', 'citadel',
        
        # äº¤æ˜“æ‰€
        'coinbase', 'binance', 'kraken', 'ftx', 'okx', 'huobi', 'bybit',
        'gemini', 'bitfinex', 'bitstamp',
        
        # æœºæ„/å…¬å¸
        'microstrategy', 'tesla', 'square', 'paypal', 'visa', 'mastercard',
        
        # é‡‘èäº§å“
        'etf', 'futures', 'options', 'derivatives', 'spot', 'perpetual',
        
        # æŠ€æœ¯/æ¦‚å¿µ
        'blockchain', 'layer2', 'validator', 'consensus', 'smart contract',
        'gas', 'gwei', 'wallet', 'cold wallet', 'hot wallet',
        
        # ç›‘ç®¡/æ”¿ç­–
        'regulation', 'compliance', 'ban', 'approval', 'license', 'cbdc',
        'money laundering', 'kyc', 'aml'
    }
    
    # ä»·æ ¼ç›¸å…³è¯
    PRICE_WORDS = {
        'surge', 'pump', 'moon', 'ath', 'all-time high', 'high', 'rally', 'gain', 'up', 'rise',
        'crash', 'dump', 'dip', 'drop', 'fall', 'down', 'loss', 'low', 'plunge', 'decline'
    }
    
    # é‡‘èç›¸å…³å…³é”®è¯ï¼ˆé‡ç‚¹å…³æ³¨å®è§‚æ”¿ç­–ï¼‰
    FINANCE_KEYWORDS = {
        # ç¾è”å‚¨ç›¸å…³ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        'fed', 'federal reserve', 'powell', 'jerome powell', 'fomc', 'federal open market',
        'interest rate', 'rate hike', 'rate cut', 'rate decision', 'fed meeting',
        'monetary policy', 'quantitative easing', 'qe', 'taper', 'tapering',
        'fed minutes', 'dot plot', 'fed fund rate',
        
        # ä¸­ç¾å…³ç³»ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        'china', 'us-china', 'sino-us', 'trade war', 'tariff', 'tariffs',
        'china us', 'beijing', 'washington', 'xi jinping', 'biden', 'trump',
        'export control', 'sanctions', 'import', 'export', 'trade deal',
        'semiconductor', 'tech war', 'huawei', 'taiwan', 'strait',
        
        # å…³ç¨è´¸æ˜“æ”¿ç­–ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰
        'customs', 'duty', 'trade policy', 'protectionism', 'free trade',
        'trade agreement', 'wto', 'trade deficit', 'trade surplus',
        
        # é€šèƒ€ä¸ç»æµæŒ‡æ ‡
        'inflation', 'cpi', 'pce', 'deflation', 'stagflation',
        'gdp', 'unemployment', 'job report', 'nonfarm payroll',
        'recession', 'economic growth', 'consumer confidence',
        
        # è´§å¸ä¸å¤–æ±‡
        'dollar', 'dxy', 'dollar index', 'currency', 'yuan', 'rmb', 'renminbi',
        'exchange rate', 'forex', 'devaluation', 'appreciation',
        
        # å€ºåˆ¸ä¸åˆ©ç‡
        'treasury', 'bond', 'yield', 'yield curve', '10-year', 'bond market',
        
        # å…¶ä»–å¤®è¡Œ
        'ecb', 'european central bank', 'lagarde',
        'bank of japan', 'boj', 'bank of england', 'boe',
        'pboc', 'people bank of china'
    }
    
    # é«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆå½±å“æœ€å¤§çš„ï¼‰
    HIGH_PRIORITY_KEYWORDS = {
        'fed', 'federal reserve', 'powell', 'fomc', 'rate hike', 'rate cut',
        'tariff', 'tariffs', 'china', 'us-china', 'trade war',
        'inflation', 'cpi', 'interest rate'
    }
    
    def __init__(self, enable_translation=True):
        """
        åˆå§‹åŒ–
        
        Args:
            enable_translation: æ˜¯å¦å¯ç”¨ç¿»è¯‘
        """
        self.enable_translation = enable_translation and TRANSLATION_AVAILABLE
        
        if enable_translation and not TRANSLATION_AVAILABLE:
            logger.warning("ç¿»è¯‘å·²ç¦ç”¨ï¼šgoogletransæœªå®‰è£…")
        
        self.translator = Translator() if self.enable_translation else None
        self.translation_cache = {}  # ç¿»è¯‘ç¼“å­˜
    
    def is_relevant_news(self, text):
        """
        åˆ¤æ–­æ–°é—»æ˜¯å¦ä¸é‡‘è/åŠ å¯†è´§å¸ç›¸å…³
        
        Args:
            text: æ–°é—»æ–‡æœ¬ï¼ˆæ ‡é¢˜+æè¿°ï¼‰
        
        Returns:
            True=ç›¸å…³, False=æ— å…³
        """
        if not text:
            return False
        
        text_lower = text.lower()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åŠ å¯†è´§å¸å…³é”®è¯
        crypto_match = any(kw in text_lower for kw in self.CRYPTO_KEYWORDS)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«é‡‘èå…³é”®è¯
        finance_match = any(kw in text_lower for kw in self.FINANCE_KEYWORDS)
        
        # ä»»ä¸€åŒ¹é…å³è®¤ä¸ºç›¸å…³
        return crypto_match or finance_match
    
    def extract_keywords(self, text, max_keywords=5):
        """
        æå–åŠ å¯†è´§å¸å’Œé‡‘èå…³é”®è¯ï¼ˆä¼˜å…ˆé«˜ä¼˜å…ˆçº§ï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            max_keywords: æœ€å¤§å…³é”®è¯æ•°
        
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        if not text:
            return []
        
        text_lower = text.lower()
        found_keywords = []
        
        # æŸ¥æ‰¾åŠ å¯†è´§å¸å…³é”®è¯
        for keyword in self.CRYPTO_KEYWORDS:
            if keyword in text_lower:
                count = text_lower.count(keyword)
                weight = 1
                
                # ä»·æ ¼ç›¸å…³è¯åŠ æƒ
                if keyword in self.PRICE_WORDS:
                    weight = 2
                
                found_keywords.append((keyword, count * weight, 'crypto'))
        
        # æŸ¥æ‰¾é‡‘èå…³é”®è¯
        for keyword in self.FINANCE_KEYWORDS:
            if keyword in text_lower:
                count = text_lower.count(keyword)
                weight = 1
                
                # é«˜ä¼˜å…ˆçº§å…³é”®è¯ï¼ˆç¾è”å‚¨ã€ä¸­ç¾ã€å…³ç¨ï¼‰åŠ æƒ
                if keyword in self.HIGH_PRIORITY_KEYWORDS:
                    weight = 3  # 3å€æƒé‡
                
                found_keywords.append((keyword, count * weight, 'finance'))
        
        # æŒ‰åŠ æƒé¢‘ç‡æ’åº
        found_keywords.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›å‰Nä¸ªï¼ˆåªè¿”å›å…³é”®è¯ï¼‰
        return [kw[0] for kw in found_keywords[:max_keywords]]
    
    def translate_to_english(self, text, source_lang='zh-cn'):
        """
        ç¿»è¯‘ä¸ºè‹±æ–‡
        
        Args:
            text: å¾…ç¿»è¯‘æ–‡æœ¬
            source_lang: æºè¯­è¨€
        
        Returns:
            è‹±æ–‡æ–‡æœ¬
        """
        if not text or not self.enable_translation:
            return text
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{source_lang}:{text[:100]}"
        if cache_key in self.translation_cache:
            return self.translation_cache[cache_key]
        
        try:
            result = self.translator.translate(text, src=source_lang, dest='en')
            translated = result.text
            
            # ç¼“å­˜ç»“æœ
            self.translation_cache[cache_key] = translated
            
            logger.info(f"ç¿»è¯‘: {text[:30]}... â†’ {translated[:30]}...")
            return translated
            
        except Exception as e:
            logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
            return text  # è¿”å›åŸæ–‡
    
    def detect_language(self, text):
        """
        æ£€æµ‹æ–‡æœ¬è¯­è¨€
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            è¯­è¨€ä»£ç  ('zh'/'en'/...)
        """
        if not text:
            return 'en'
        
        # ç®€å•æ£€æµ‹ï¼šæ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
        if re.search(r'[\u4e00-\u9fff]', text):
            return 'zh'
        else:
            return 'en'
    
    def extract_key_sentence(self, text, max_length=100):
        """
        æå–å…³é”®å¥å­ï¼ˆæ‘˜è¦ï¼‰
        
        Args:
            text: æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
        
        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        if not text:
            return ""
        
        # æŒ‰å¥å­åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', text)
        
        if not sentences:
            return text[:max_length]
        
        # è¯„åˆ†ï¼šåŒ…å«å…³é”®è¯çš„å¥å­ä¼˜å…ˆ
        scored_sentences = []
        for sent in sentences:
            if len(sent.strip()) < 10:  # å¤ªçŸ­çš„è·³è¿‡
                continue
            
            score = 0
            sent_lower = sent.lower()
            
            # å…³é”®è¯åŠ åˆ†
            for keyword in self.CRYPTO_KEYWORDS:
                if keyword in sent_lower:
                    score += 1
            
            # ä»·æ ¼è¯åŠ åˆ†
            for word in self.PRICE_WORDS:
                if word in sent_lower:
                    score += 2
            
            scored_sentences.append((sent.strip(), score))
        
        if not scored_sentences:
            return text[:max_length]
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„å¥å­
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        best_sentence = scored_sentences[0][0]
        
        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
        if len(best_sentence) > max_length:
            best_sentence = best_sentence[:max_length] + "..."
        
        return best_sentence
    
    def process_single_news(self, news_item):
        """
        å¤„ç†å•æ¡æ–°é—»
        
        Args:
            news_item: {
                'title': 'æ ‡é¢˜',
                'description': 'æè¿°',
                'source': 'æ¥æº',
                'published_at': 'æ—¶é—´',
                'language': 'zh'|'en' (å¯é€‰)
            }
        
        Returns:
            å¤„ç†åçš„æ–°é—»
        """
        title = news_item.get('title', '')
        desc = news_item.get('description', '')
        source = news_item.get('source', '')
        
        # æ£€æµ‹è¯­è¨€
        lang = news_item.get('language')
        if not lang:
            lang = self.detect_language(title)
        
        # åŸå§‹å†…å®¹
        original_title = title
        original_desc = desc
        
        # 1. ç¿»è¯‘ï¼ˆå¦‚æœæ˜¯ä¸­æ–‡ï¼‰
        if lang == 'zh' and self.enable_translation:
            try:
                title = self.translate_to_english(title, 'zh-cn')
                if desc:
                    desc = self.translate_to_english(desc, 'zh-cn')
            except Exception as e:
                logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
        
        # 2. æå–å…³é”®è¯
        full_text = f"{title} {desc}"
        keywords = self.extract_keywords(full_text, max_keywords=5)
        
        # 3. æå–å…³é”®å¥ï¼ˆæ‘˜è¦ï¼‰
        summary = self.extract_key_sentence(desc if desc else title, max_length=100)
        
        # 4. è®¡ç®—Tokenæ•°ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
        simplified_text = f"{title} {' '.join(keywords)} {summary}"
        token_count = len(simplified_text.split())
        
        processed = {
            'original_title': original_title,
            'original_description': original_desc,
            'title': title,
            'description': desc,
            'keywords': keywords,
            'summary': summary,
            'source': source,
            'published_at': news_item.get('published_at', ''),
            'language': lang,
            'token_count': token_count
        }
        
        return processed
    
    def process_news_list(self, news_list, filter_irrelevant=True):
        """
        æ‰¹é‡å¤„ç†æ–°é—»
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            filter_irrelevant: æ˜¯å¦è¿‡æ»¤æ— å…³æ–°é—»
        
        Returns:
            å¤„ç†åçš„æ–°é—»åˆ—è¡¨
        """
        processed_list = []
        filtered_count = 0
        
        for news in news_list:
            try:
                # è¿‡æ»¤ï¼šåªä¿ç•™é‡‘è/åŠ å¯†è´§å¸ç›¸å…³æ–°é—»
                if filter_irrelevant:
                    title = news.get('title', '')
                    desc = news.get('description', '')
                    full_text = f"{title} {desc}"
                    
                    if not self.is_relevant_news(full_text):
                        filtered_count += 1
                        logger.debug(f"è¿‡æ»¤æ— å…³æ–°é—»: {title[:50]}...")
                        continue
                
                processed = self.process_single_news(news)
                processed_list.append(processed)
            except Exception as e:
                logger.error(f"å¤„ç†æ–°é—»å¤±è´¥: {e}")
                continue
        
        logger.info(f"æˆåŠŸå¤„ç† {len(processed_list)}/{len(news_list)} æ¡æ–°é—» (è¿‡æ»¤ {filtered_count} æ¡æ— å…³)")
        return processed_list
    
    def generate_compact_prompt(self, processed_news_list, max_news=10):
        """
        ç”Ÿæˆç´§å‡‘çš„AI Promptï¼ˆçœTokenï¼‰
        
        Args:
            processed_news_list: å¤„ç†åçš„æ–°é—»åˆ—è¡¨
            max_news: æœ€å¤šåŒ…å«æ–°é—»æ•°
        
        Returns:
            ç´§å‡‘çš„Promptæ–‡æœ¬
        """
        if not processed_news_list:
            return "No recent news"
        
        # åªå–æœ€æ–°çš„Næ¡
        news_subset = processed_news_list[:max_news]
        
        # æ”¶é›†æ‰€æœ‰å…³é”®è¯
        all_keywords = []
        for news in news_subset:
            all_keywords.extend(news.get('keywords', []))
        
        # å»é‡å¹¶ç»Ÿè®¡é¢‘ç‡
        keyword_freq = {}
        for kw in all_keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åº
        top_keywords = sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)
        top_keywords = [kw[0] for kw in top_keywords[:10]]
        
        # æ ¼å¼1: è¶…ç²¾ç®€ç‰ˆï¼ˆä»…å…³é”®è¯+æ•°é‡ï¼‰
        prompt = f"News: {len(news_subset)} items, Hot topics: {', '.join(top_keywords)}\n"
        
        # æ ¼å¼2: æ ‡é¢˜+å…³é”®è¯ï¼ˆå‰5æ¡ï¼‰
        prompt += "Headlines:\n"
        for i, news in enumerate(news_subset[:5], 1):
            keywords_str = ','.join(news.get('keywords', [])[:3])
            title = news['title'][:60]  # æˆªæ–­æ ‡é¢˜
            prompt += f"{i}. {title}... [{keywords_str}]\n"
        
        return prompt
    
    def generate_detailed_summary(self, processed_news_list, max_news=5):
        """
        ç”Ÿæˆè¯¦ç»†æ‘˜è¦ï¼ˆåŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
        
        Args:
            processed_news_list: å¤„ç†åçš„æ–°é—»åˆ—è¡¨
            max_news: æœ€å¤šåŒ…å«æ–°é—»æ•°
        
        Returns:
            è¯¦ç»†æ‘˜è¦æ–‡æœ¬
        """
        if not processed_news_list:
            return "No recent news available"
        
        news_subset = processed_news_list[:max_news]
        
        summary = f"ğŸ“° Latest {len(news_subset)} News Updates:\n\n"
        
        for i, news in enumerate(news_subset, 1):
            summary += f"[{i}] {news['title']}\n"
            summary += f"    Keywords: {', '.join(news.get('keywords', []))}\n"
            summary += f"    Summary: {news.get('summary', 'N/A')}\n"
            summary += f"    Source: {news['source']} | {news['published_at'][:10]}\n\n"
        
        return summary


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•
    processor = NewsProcessor(enable_translation=True)
    
    # æµ‹è¯•æ–°é—»
    test_news = {
        'title': 'æŸå·¨é²¸è¿‡å»5å¤©ä»Krakenå›¤ç§¯894æšBTCä»·å€¼1.02äº¿ç¾å…ƒ',
        'description': 'æ®Lookonchainç›‘æµ‹ï¼ŒæŸå·¨é²¸åœ°å€åœ¨è¿‡å»5å¤©å†…ä»Krakenæå–äº†894æšBTC',
        'source': 'Odaily',
        'published_at': '2024-10-28',
        'language': 'zh'
    }
    
    print("åŸå§‹æ–°é—»:")
    print(f"  æ ‡é¢˜: {test_news['title']}")
    print()
    
    processed = processor.process_single_news(test_news)
    
    print("å¤„ç†å:")
    print(f"  è‹±æ–‡æ ‡é¢˜: {processed['title']}")
    print(f"  å…³é”®è¯: {processed['keywords']}")
    print(f"  æ‘˜è¦: {processed['summary']}")
    print(f"  Tokenæ•°: {processed['token_count']}")
    print()
    
    # æµ‹è¯•æ‰¹é‡å¤„ç†
    news_list = [test_news] * 3
    processed_list = processor.process_news_list(news_list)
    
    print("ç´§å‡‘Prompt:")
    print(processor.generate_compact_prompt(processed_list))
